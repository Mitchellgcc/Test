🧠 THE FULLY UPDATED AUTONOMOUS CURSOR MICRO-WORKFLOW (WITH FULL VISIBILITY + CI DEPLOYS)

🛰️ STEP 0: VISIBILITY & CI PREVIEW SET-UP

# HumanToDo

Do this once to ensure you can monitor & interact with your builds entirely from a phone.
✅ 0.1 Cursor Web + Mobile
Go to https://cursor.com/agents
Tap "Add to Home Screen" to install as mobile app (PWA)
Enable Push Notifications in browser
✅ 0.2 Slack Bot Integration
/apps install cursor
/cursor link https://github.com/YOUR_ORG/YOUR_REPO
Mention @Cursor in a thread to spawn/resume agents
Setup Slack webhook URL (for agent output → Slack messages)
✅ 0.3 GitHub Mobile
Install GitHub app (iOS/Android)
Enable PR notifications
Star your repo for easy access
✅ 0.4 Preview Deploys via Vercel (or Netlify)
Connect repo to Vercel
Enable Preview Deployments per PR
Optional: add CLI deploy for final push

# CursorToDo
🔧 STEP 1: PROJECT INFRASTRUCTURE SETUP

✅ 1.1 Create .cursor/environment.json
Tell Cursor how to install, start, and test inside the remote agent VM:

{
  "install": "rm -rf node_modules && pnpm install --prefer-offline --frozen-lockfile",
  "terminals": [
    { "name": "dev",    "command": "pnpm dev" },
    { "name": "test",   "command": "pnpm test" },
    { "name": "build",  "command": "pnpm build" },
    { "name": "lint",   "command": "pnpm lint" },
    { "name": "deploy", "command": "vercel --token $VERCEL_TOKEN --yes" }
  ]
}
Add VERCEL_TOKEN (or NETLIFY_AUTH_TOKEN) and SLACK_WEBHOOK under:
Cursor UI → Project → Secrets
✅ 1.2 Define .cursor/rules
Injected into every agent prompt:

- Use TypeScript for all backend and frontend logic
- Avoid editing any files in /legacy or /vendor
- Always generate unit and integration tests with new features
- Use async/await only, never .then()
- Follow domain-driven naming conventions (e.g., loginController)
- Keep directory well-structured, build folders where necessary
- Always add parseable terminal output to test runners:
  - console.log("✅ TESTS PASSED") for success
  - console.log("❌ TEST FAILURE") for failures
- Run tests via pnpm test and show clear ✅ or ❌ status
- Add extra logging during testing for maximum visibility
- When tests fail, keep adding logging until issues are identified and fixed
- Always check if a server/dev process is running before starting a new one
- Use chat.completions.create for OpenAI API calls, not deprecated endpoints
- For web projects: use dark mode, colorful DaisyUI, Tailwind, and anime.js
- Include nice waiting animations for user-facing operations
- Always run tests live unless specifically asked to mock
- After opening a PR, always run the DEPLOY & NOTIFY step
- All preview URLs must be echoed as: 🌐 PREVIEW_URL=<url>
- After REFLECT, always execute JOURNAL then PUSH
- The JOURNAL entry must be ≤ 20 lines
- Always list modified paths relative to repo root
- Before starting work, read docs/agent-log.md for previous agent context
✅ 1.3 Git Setup
git init
git remote add origin https://github.com/YOUR_USERNAME/YOUR_REPO.git
git branch -m main
git add .
git commit -m "Initial setup: Autonomous Cursor workflow infrastructure"
git push -u origin main
🚨 Agents require Git branches to push PRs
✅ 1.4 Create .gitignore
# Dependencies
node_modules/
# Builds
dist/
build/
out/
# Env
.env*
# OS
.DS_Store
# IDE
.vscode/
.idea/
✅ 1.5 Create docs/agent-log.md
mkdir -p docs
echo "# Agent Handoff Log\n\nContext persistence for autonomous agent workflows.\n" > docs/agent-log.md
git add docs/agent-log.md && git commit -m "feat: add agent handoff log" && git push
✅ 1.6 Persist "Memories" in Cursor
Go to Cursor IDE > Right Panel > Memories
Add:

"We use JWT for authentication, not sessions."
"UI is built with React + Tailwind."
"Tests must run via pnpm test and show ✅ or ❌."
"Deploy previews go through Vercel, echoing 🌐 PREVIEW_URL=<url>."
"Always read docs/agent-log.md before starting work for context from previous agents."

🗂 STEP 2: PREPARE DAILY TASK PLAN (external)

Cursor doesn't read .md files — you paste each task into the agent queue.
Example:

## Agent: analytics-ui-agent

1. Create /analytics React page  
2. Add chart props and dummy bar chart  
3. Build 3 test cases (render, empty, failure)  
4. Run pnpm test  
5. Retry on ❌  
6. Submit PR

### REFLECT
Summarise progress  
Suggest next 3 tasks  
Output them as ### NEXT_QUEUE

### JOURNAL
1. Read docs/agent-log.md (create if missing)
2. Append:

## <AGENT_NAME> – <YYYY-MM-DD HH:MM>
- Scope: <high-level goal>
- Key files touched: <list>
- Tests: ✅ or ❌
- Preview: <url>
- Next tasks: <3 bullet points>

3. Save file

### DEPLOY
Run `vercel --token $VERCEL_TOKEN --yes`  
Echo 🌐 PREVIEW_URL=<url>

### NOTIFY
POST preview URL to $SLACK_WEBHOOK

### PUSH
git add docs/agent-log.md
git commit -m "chore(log): journal update ${AGENT_NAME} ✅ TESTS PASSED 🌐 PREVIEW_URL=<url>"
git push --set-upstream origin <branch>
gh pr create --fill

### EXIT_ON_SUCCESS
🤖 STEP 3: SPAWN & QUEUE AGENTS

✅ 3.1 Open Cursor + Wait for Project Indexing
✅ 3.2 Spawn Background Agent
Ctrl + E → Background Agent
Name it (e.g., analytics-api-agent)
Paste: "Review docs/agent-log.md for context from previous agents, then continue with:"
Paste initial prompt from backlog
✅ 3.3 Queue Prompts
Paste remaining queue prompts, ending with REFLECT, JOURNAL, DEPLOY, NOTIFY, PUSH, and EXIT_ON_SUCCESS.

Monitor agent in Cursor Web, Slack, or desktop IDE.

🧪 STEP 4: ENABLE SELF-MONITORING & RECOVERY

✅ 4.1 In Test Runners
if (passed) {
  console.log("✅ TESTS PASSED")
} else {
  console.log("❌ TEST FAILURE")
}
✅ 4.2 In Prompts
Run pnpm test  
If output contains ❌, regenerate broken tests and retry once  
Only open PR on ✅
🔁 STEP 5: CODE REVIEW & MERGE

✅ 5.1 BugBot Review
PR auto-reviewed
Inline comments + "Fix in Cursor" button
✅ 5.2 Optional Review Agent
1. Checkout PR #123  
2. Run tests  
3. Patch if lint or coverage < 90%  
4. Approve PR if clean
✅ 5.3 Merge via GitHub Mobile
Use squash-merge or main-branch promotion
CI will deploy automatically

🧼 STEP 6: DAILY RESET

✅ 6.1 Clean Branches
After merging, delete stale branches if not auto-deleted

✅ 6.2 Archive Task Logs
Rename task file: tasks-2025-07-03.md
Use it to compare REFLECT output from agent vs plan

🧭 OPTIONAL DAILY SCHEDULE TEMPLATE

Time	Action
08:30	Write task prompts + goals
09:00	Spawn agents and queue
09:15–12:00	Cursor builds/tests/reflects
12:00	Check preview links in Slack
13:00	Review PRs on GitHub Mobile
14:00–17:00	Queue next feature set
17:00–18:00	Merge PRs, cleanup, archive
🛠 COMMON ISSUES & FIXES

Issue	Fix
Agent stuck on Y/n prompt	Ensure all install/dev/test commands are non-interactive
Agent can't deploy	Add $VERCEL_TOKEN as project secret
Preview URL not found	Ensure agent echoes 🌐 PREVIEW_URL=<url> and doesn't suppress output
No Slack pings	Check $SLACK_WEBHOOK is set + added in queue
Agent tool-call limit hit	Use Max Mode or add @Cursor resume agent-name in Slack
Agent ignores previous context	Ensure "Review docs/agent-log.md" is first in every agent prompt
Context handoff broken	Check docs/agent-log.md exists and agents are appending (not overwriting)

🎯 QUICK COPY-PASTE AGENT QUEUE TAIL

### REFLECT
Summarise work completed in 3-5 lines
List 3 NEXT_QUEUE tasks for subsequent agents

### JOURNAL
echo "## ${AGENT_NAME} – $(date -Iseconds)" >> docs/agent-log.md
echo "- Scope: [describe high-level goal]" >> docs/agent-log.md  
echo "- Files: [list key paths touched]" >> docs/agent-log.md
echo "- Tests: ${TEST_STATUS}" >> docs/agent-log.md
echo "- Preview: ${PREVIEW_URL}" >> docs/agent-log.md
echo "- Next: [3 bullet points from REFLECT]" >> docs/agent-log.md
echo "" >> docs/agent-log.md

### DEPLOY
vercel --token $VERCEL_TOKEN --yes
echo "🌐 PREVIEW_URL=$(cat .vercel/preview.txt)"

### NOTIFY  
curl -X POST $SLACK_WEBHOOK -d '{"text":"🚀 '${AGENT_NAME}' deployed: '${PREVIEW_URL}'"}'

### PUSH
git add docs/agent-log.md
git commit -m "chore(log): ${AGENT_NAME} journal ✅ ${TEST_STATUS} 🌐 ${PREVIEW_URL}"
git push --set-upstream origin $(git branch --show-current)
gh pr create --fill

### EXIT_ON_SUCCESS
